{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97919,"databundleVersionId":11872932,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install necessary libraries quietly\n!pip install openai-whisper sentence-transformers -q\nprint(\"Required libraries installation check complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Imports & Setup ---\nimport os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport soundfile as sf\nimport whisper # OpenAI Whisper for ASR\nfrom tqdm.notebook import tqdm # Progress bar for loops\n\n# Sentence Embeddings\nfrom sentence_transformers import SentenceTransformer\n\n# Scikit-learn imports\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer # Keep for reference, but won't be used\nimport xgboost as xgb # Using XGBoost as the regressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Optional: Set a random seed for reproducibility\nSEED = 42\nnp.random.seed(SEED)\nprint(\"Libraries imported.\")\n\n# --- Define Paths Based on Your Kaggle Folder Structure ---\nBASE_INPUT_DIR = '/kaggle/input/shl-intern-hiring-assessment/Dataset/'\n\nTRAIN_CSV = os.path.join(BASE_INPUT_DIR, 'train.csv')\nTEST_CSV = os.path.join(BASE_INPUT_DIR, 'test.csv')\nSAMPLE_SUBMISSION_CSV = os.path.join(BASE_INPUT_DIR, 'sample_submission.csv')\n\nAUDIO_BASE_DIR = os.path.join(BASE_INPUT_DIR, 'audios/')\nAUDIO_TRAIN_DIR = os.path.join(AUDIO_BASE_DIR, 'train/')\nAUDIO_TEST_DIR = os.path.join(AUDIO_BASE_DIR, 'test/')\n\n# Define output directory (Kaggle uses /kaggle/working/)\nOUTPUT_DIR = '/kaggle/working/'\nTRANSCRIPTIONS_DIR = os.path.join(OUTPUT_DIR, 'transcriptions/') # To save transcriptions\nSUBMISSION_FILE = os.path.join(OUTPUT_DIR, 'submission.csv')    # Where final submission goes\n\n# Create transcriptions directory if it doesn't exist in the working folder\nos.makedirs(TRANSCRIPTIONS_DIR, exist_ok=True)\n\nprint(\"\\n--- Paths Defined ---\")\nprint(f\"Train CSV: {TRAIN_CSV}\")\nprint(f\"Test CSV: {TEST_CSV}\")\nprint(f\"Sample Submission: {SAMPLE_SUBMISSION_CSV}\")\nprint(f\"Train Audio Dir: {AUDIO_TRAIN_DIR}\")\nprint(f\"Test Audio Dir: {AUDIO_TEST_DIR}\")\nprint(f\"Output/Working Dir: {OUTPUT_DIR}\")\nprint(f\"Transcription Cache Dir: {TRANSCRIPTIONS_DIR}\")\nprint(f\"Submission File Output Path: {SUBMISSION_FILE}\")\n\n# --- Define Target Sample Rate ---\n# Whisper models work best with 16kHz audio\nTARGET_SR = 16000\nprint(f\"\\nTarget Sample Rate for audio processing: {TARGET_SR} Hz\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Load Data ---\ntry:\n    train_df = pd.read_csv(TRAIN_CSV)\n    test_df = pd.read_csv(TEST_CSV)\n    sample_sub = pd.read_csv(SAMPLE_SUBMISSION_CSV)\n\n    print(\"--- Training Data ---\")\n    print(f\"Shape: {train_df.shape}\")\n    print(train_df.head())\n    print(\"\\nInfo:\")\n    train_df.info()\n    print(\"\\nMissing values:\")\n    print(train_df.isnull().sum())\n\n\n    print(\"\\n--- Test Data ---\")\n    print(f\"Shape: {test_df.shape}\")\n    print(test_df.head()) # Note: Test label/score column is placeholder\n    print(\"\\nInfo:\")\n    test_df.info()\n    print(\"\\nMissing values:\")\n    print(test_df.isnull().sum())\n\n    print(\"\\n--- Sample Submission ---\")\n    print(f\"Shape: {sample_sub.shape}\")\n    print(sample_sub.head()) # Shows expected 'score' column name for output\n\nexcept FileNotFoundError as e:\n    print(f\"Error loading CSV files: {e}\")\n    print(\"Please double-check the BASE_INPUT_DIR and filenames in Cell 2.\")\n    raise # Stop execution if files aren't found","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2.1 Set Score Column Name ---\n\n# Based on the previous error message \"columns (['filename', 'label'])\",\n# the correct column name is 'label'. We set it directly here.\n\nSCORE_COLUMN_NAME = 'label' # <--- SETTING CORRECT NAME BASED ON ERROR\n\n# This part checks if the name we set actually exists in the DataFrame.\nprint(\"\\nVerifying score column name...\")\nif SCORE_COLUMN_NAME not in train_df.columns:\n    # If you see this error, something unexpected happened with the CSV loading.\n    raise ValueError(f\"ERROR: Column '{SCORE_COLUMN_NAME}' was expected but NOT found in train_df columns ({train_df.columns.tolist()}). Check CSV loading in Cell 3.\")\nelse:\n    # This confirmation message should appear when you run this cell.\n    print(f\"CONFIRMED: Using column '{SCORE_COLUMN_NAME}' for grammar scores.\")\n    print(\"Proceed to the next cell (Cell 5).\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Explore Score Distribution (Training Data) ---\n# This cell uses SCORE_COLUMN_NAME which is now correctly set to 'label'.\n\nprint(f\"\\n--- Analysis of Score Column: '{SCORE_COLUMN_NAME}' ---\")\nprint(f\"Description:\")\nprint(train_df[SCORE_COLUMN_NAME].describe())\n\n# Check data type\nprint(f\"\\nData type: {train_df[SCORE_COLUMN_NAME].dtype}\")\nif not pd.api.types.is_numeric_dtype(train_df[SCORE_COLUMN_NAME]):\n      print(f\"Warning: Score column '{SCORE_COLUMN_NAME}' is not numeric. Model training might fail.\")\n\n\nplt.figure(figsize=(10, 5))\n# Use the variable for the score column\ntry:\n    # Calculate bins based on min/max score\n    min_score = train_df[SCORE_COLUMN_NAME].min()\n    max_score = train_df[SCORE_COLUMN_NAME].max()\n    if pd.api.types.is_integer_dtype(train_df[SCORE_COLUMN_NAME]):\n        num_bins = int(max_score - min_score + 1) # One bin per integer value\n    else:\n        num_bins = max(10, int(np.ceil(max_score) - np.floor(min_score))*2 + 1) # Heuristic for float scores\n\n    sns.histplot(train_df[SCORE_COLUMN_NAME], kde=True, bins=num_bins)\n    plt.title(f'Distribution of Grammar Scores ({SCORE_COLUMN_NAME}) in Training Data')\n    plt.xlabel(f'Grammar Score ({SCORE_COLUMN_NAME})')\n    plt.ylabel('Frequency')\n    plt.grid(axis='y', alpha=0.5)\n    plt.show()\nexcept Exception as e:\n    print(f\"Could not plot histogram for column '{SCORE_COLUMN_NAME}': {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4. Audio Exploration (Sample) ---\ndef check_audio_properties(audio_dir, filenames, num_samples=3):\n    \"\"\"Checks basic properties of a few audio files.\"\"\"\n    print(f\"\\nChecking properties of up to {num_samples} audio files in {audio_dir}...\")\n    if not os.path.exists(audio_dir):\n        print(f\"ERROR: Audio directory not found: {audio_dir}\")\n        return\n\n    if isinstance(filenames, pd.Series):\n        filenames_list = filenames.dropna().tolist()\n    elif isinstance(filenames, (list, np.ndarray)):\n         filenames_list = [f for f in filenames if isinstance(f, str)]\n    else:\n        print(f\"Warning: Unexpected type for filenames: {type(filenames)}. Attempting to iterate.\")\n        filenames_list = filenames\n\n    if not filenames_list:\n         print(\"Filename sequence is empty or contains no valid names.\")\n         return\n\n    processed_count = 0\n    for filename in filenames_list:\n        if processed_count >= num_samples:\n            break\n        if not isinstance(filename, str) or not filename.lower().endswith('.wav'):\n            print(f\"  Skipping invalid filename entry: {filename}\")\n            continue\n\n        filepath = os.path.join(audio_dir, filename)\n        if not os.path.exists(filepath):\n             print(f\"  WARNING: File not found: {filepath}\")\n             continue\n        try:\n            info = sf.info(filepath)\n            print(f\"File: {filename}\")\n            print(f\"  Duration: {info.duration:.2f} seconds\")\n            print(f\"  Sample Rate: {info.samplerate} Hz\")\n            print(f\"  Channels: {info.channels}\")\n            if info.samplerate != TARGET_SR:\n                print(f\"    -> Note: Sample rate ({info.samplerate} Hz) differs from target ({TARGET_SR} Hz). Resampling needed.\")\n            if info.channels > 1:\n                print(f\"    -> Note: {info.channels} channels detected. Will be converted to mono.\")\n            processed_count += 1\n        except Exception as e:\n            print(f\"ERROR reading properties of {filename}: {e}\")\n\n# Check a few training files\ncheck_audio_properties(AUDIO_TRAIN_DIR, train_df['filename'])\n# Check a few test files\ncheck_audio_properties(AUDIO_TEST_DIR, test_df['filename'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. Automatic Speech Recognition (ASR) Setup ---\n\n# Define Whisper model size. Options: 'tiny', 'base', 'small', 'medium', 'large'\n# RECOMMENDATION: Use 'small' for better accuracy than 'base' if time permits.\nASR_MODEL_SIZE = \"small\" # <<< CHANGED TO SMALL (can change back to 'base' if needed)\n\ntry:\n    import torch\n    if torch.cuda.is_available():\n        device = \"cuda\"\n        print(f\"GPU ({torch.cuda.get_device_name(0)}) detected. Using CUDA for Whisper.\")\n    else:\n        device = \"cpu\"\n        print(\"CUDA not available. Using CPU for Whisper (this will be significantly slower).\")\n\n    print(f\"Loading Whisper model: '{ASR_MODEL_SIZE}'...\")\n    whisper_model = whisper.load_model(ASR_MODEL_SIZE, device=device)\n    print(f\"Whisper model ('{ASR_MODEL_SIZE}') loaded successfully onto {device}.\")\nexcept ImportError:\n     print(\"ERROR: PyTorch not found. Whisper requires PyTorch.\")\n     raise\nexcept Exception as e:\n    print(f\"ERROR loading Whisper model: {e}\")\n    raise","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 6. Transcribe Audio Files ---\n\ndef transcribe_audio(filepath, model):\n    \"\"\"Loads, resamples, converts to mono, and transcribes a single audio file.\"\"\"\n    if not os.path.exists(filepath):\n        print(f\"Warning: File not found during transcription: {filepath}\")\n        return None\n    try:\n        audio, sr = librosa.load(filepath, sr=TARGET_SR, mono=True)\n        audio = audio.astype(np.float32)\n        result = model.transcribe(audio, fp16=(device == 'cuda'))\n        transcription_text = result.get('text', '')\n        return transcription_text.strip()\n    except Exception as e:\n        print(f\"ERROR during transcription of {os.path.basename(filepath)}: {e}\")\n        return \"\"\n\n# --- Define Cache File Paths (include model size) ---\ntrain_transcriptions_file = os.path.join(TRANSCRIPTIONS_DIR, f'train_transcriptions_{ASR_MODEL_SIZE}.csv')\ntest_transcriptions_file = os.path.join(TRANSCRIPTIONS_DIR, f'test_transcriptions_{ASR_MODEL_SIZE}.csv')\nprint(f\"\\nTranscription cache paths:\")\nprint(f\"  Train: {train_transcriptions_file}\")\nprint(f\"  Test: {test_transcriptions_file}\")\n# --- OPTIONAL: Force re-transcription by uncommenting the lines below ---\n# print(\"DEBUG: Forcing re-transcription by removing cache files (if they exist)...\")\n# if os.path.exists(train_transcriptions_file): os.remove(train_transcriptions_file)\n# if os.path.exists(test_transcriptions_file): os.remove(test_transcriptions_file)\n# ------------------------------------------------------------------------\n\n# --- Function to Process Transcriptions ---\ndef process_dataframe_transcriptions(df, audio_dir, cache_file, df_name):\n    \"\"\"Loads cached transcriptions or generates them if cache is missing.\"\"\"\n    if os.path.exists(cache_file):\n        print(f\"\\nLoading cached {df_name} transcriptions from: {cache_file}...\")\n        transcripts_df = pd.read_csv(cache_file)\n        if 'transcription' not in transcripts_df.columns: transcripts_df['transcription'] = ''\n        df = pd.merge(df, transcripts_df[['filename', 'transcription']], on='filename', how='left')\n        print(f\"Loaded {df['transcription'].notna().sum()} cached {df_name} transcriptions.\")\n    else:\n        print(f\"\\nCache file not found. Transcribing {df_name} audio files (Using {ASR_MODEL_SIZE} model - this will take time)...\")\n        transcriptions = {}\n        for filename in tqdm(df['filename'], desc=f\"Transcribing {df_name} Audio\"):\n            filepath = os.path.join(audio_dir, filename)\n            transcription = transcribe_audio(filepath, whisper_model)\n            transcriptions[filename] = transcription if transcription is not None else \"\"\n        df['transcription'] = df['filename'].map(transcriptions)\n        try:\n            df[['filename', 'transcription']].to_csv(cache_file, index=False)\n            print(f\"Saved {df_name} transcriptions to cache: {cache_file}\")\n        except Exception as e:\n            print(f\"ERROR saving transcription cache file {cache_file}: {e}\")\n    return df\n\n# --- Process Training and Test Data ---\ntrain_df = process_dataframe_transcriptions(train_df, AUDIO_TRAIN_DIR, train_transcriptions_file, \"Train\")\ntest_df = process_dataframe_transcriptions(test_df, AUDIO_TEST_DIR, test_transcriptions_file, \"Test\")\n\n# --- Final Check for Missing Transcriptions ---\ntrain_missing_transcriptions = train_df['transcription'].isnull().sum()\ntest_missing_transcriptions = test_df['transcription'].isnull().sum()\nif train_missing_transcriptions > 0:\n    print(f\"Warning: {train_missing_transcriptions} training samples still have missing transcriptions. Filling with empty string.\")\n    train_df['transcription'].fillna('', inplace=True)\nif test_missing_transcriptions > 0:\n    print(f\"Warning: {test_missing_transcriptions} test samples still have missing transcriptions. Filling with empty string.\")\n    test_df['transcription'].fillna('', inplace=True)\n\nprint(\"\\n--- Sample Training Data with Transcriptions ---\")\nprint(train_df[['filename', SCORE_COLUMN_NAME, 'transcription']].head())\nprint(\"\\n--- Sample Test Data with Transcriptions ---\")\nprint(test_df[['filename', 'transcription']].head())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 6.1 Visualize Transcription Lengths ---\nprint(\"\\nAnalyzing transcription lengths...\")\n\n# Calculate length (e.g., number of characters or words)\ntrain_df['transcription_length'] = train_df['transcription'].apply(len)\ntest_df['transcription_length'] = test_df['transcription'].apply(len)\n# Or use word count: train_df['transcription'].apply(lambda x: len(x.split()))\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nsns.histplot(train_df['transcription_length'], kde=True, bins=30)\nplt.title('Distribution of Transcription Lengths (Train)')\nplt.xlabel('Length (Characters)')\nplt.ylabel('Frequency')\n\nplt.subplot(1, 2, 2)\nsns.histplot(test_df['transcription_length'], kde=True, bins=30)\nplt.title('Distribution of Transcription Lengths (Test)')\nplt.xlabel('Length (Characters)')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Train transcription length stats:\\n{train_df['transcription_length'].describe()}\")\nprint(f\"\\nTest transcription length stats:\\n{test_df['transcription_length'].describe()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 7. Feature Engineering (Sentence Embeddings) ---\n\n# Define the Sentence Transformer model to use\nSBERT_MODEL_NAME = 'all-MiniLM-L6-v2'\nprint(f\"\\nLoading SentenceTransformer model: '{SBERT_MODEL_NAME}'...\")\n\ntry:\n    # Load the model, use GPU if available (device defined in Cell 7)\n    embedding_model = SentenceTransformer(SBERT_MODEL_NAME, device=device)\n    print(\"SentenceTransformer model loaded successfully.\")\nexcept Exception as e:\n    print(f\"ERROR loading SentenceTransformer model: {e}\")\n    raise\n\nprint(\"\\nGenerating sentence embeddings for transcriptions...\")\n# Convert transcriptions to lists\ntrain_texts = train_df['transcription'].tolist()\ntest_texts = test_df['transcription'].tolist()\n\n# Generate embeddings (this can also take some time, but usually less than ASR)\nprint(\"Encoding training data...\")\nX_train_embeddings = embedding_model.encode(train_texts, show_progress_bar=True)\n\nprint(\"Encoding test data...\")\nX_test_embeddings = embedding_model.encode(test_texts, show_progress_bar=True)\n\n# --- Define Feature and Target Variables for subsequent cells ---\nX_train_features = X_train_embeddings # Use embeddings as features\nX_test_features = X_test_embeddings\ny_train = train_df[SCORE_COLUMN_NAME] # Target remains the 'label' column\n# ----------------------------------------------------------------\n\nprint(f\"\\nShape of Sentence Embeddings (Train Features): {X_train_features.shape}\")\nprint(f\"Shape of Sentence Embeddings (Test Features): {X_test_features.shape}\")\nprint(f\"Shape of target variable '{SCORE_COLUMN_NAME}' (Train): {y_train.shape}\")\nprint(\"\\nFeature engineering using sentence embeddings complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 8. Model Training Setup: Train-Validation Split ---\n\nprint(f\"\\nSplitting training data (Embeddings shape: {X_train_features.shape}) into training and validation sets...\")\n# Split the embedding features (X_train_features) and target (y_train)\nX_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n    X_train_features,       # Use embedding features\n    y_train,                # Target scores ('label' column)\n    test_size=0.20,\n    random_state=SEED\n)\n\nprint(\"Data split complete:\")\nprint(f\"  Training features shape:   {X_train_split.shape}\")\nprint(f\"  Validation features shape: {X_val_split.shape}\")\nprint(f\"  Training labels shape:     {y_train_split.shape}\")\nprint(f\"  Validation labels shape:   {y_val_split.shape}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 9. Model Selection and Training (on split data with embeddings) ---\n\nprint(\"\\nInitializing and training XGBoost Regressor model on embedding features...\")\n\n# Initialize the XGBoost Regressor model (hyperparameters might need tuning for embeddings)\nxgb_model = xgb.XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=500,             # Increased estimators, relying more on early stopping\n    learning_rate=0.03,           # Potentially smaller learning rate for dense features\n    max_depth=4,                  # Potentially shallower depth\n    subsample=0.7,\n    colsample_bytree=0.7,\n    gamma=0.2,                    # Slightly increased gamma\n    reg_alpha=0.01,\n    random_state=SEED,\n    n_jobs=-1,\n    early_stopping_rounds=30      # Increased patience\n)\n\nprint(f\"Training with early stopping (patience={xgb_model.early_stopping_rounds})...\")\n# Train using the split embedding features\nxgb_model.fit(X_train_split, y_train_split,\n              eval_set=[(X_val_split, y_val_split)],\n              verbose=False) # Suppress per-round output\n\nprint(\"XGBoost model training complete.\")\n\nbest_iteration = xgb_model.best_iteration if hasattr(xgb_model, 'best_iteration') else None\nif best_iteration is not None:\n     print(f\"Early stopping triggered. Best number of rounds: {best_iteration + 1}\")\n     best_score = xgb_model.best_score if hasattr(xgb_model, 'best_score') else \"N/A\"\n     print(f\"Best validation score (RMSE): {best_score:.4f}\")\nelse:\n     print(\"Early stopping did not trigger. Model trained for full n_estimators.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 10. Evaluation on Validation Set ---\n\nprint(\"\\nEvaluating trained XGBoost model on the validation set (embeddings)...\")\n# Predict on the validation embedding features\nval_preds_xgb = xgb_model.predict(X_val_split)\n\n# Calculate evaluation metrics\nrmse_xgb_val = mean_squared_error(y_val_split, val_preds_xgb, squared=False) # RMSE\nmae_xgb_val = mean_absolute_error(y_val_split, val_preds_xgb)             # MAE\n\nprint(f\"\\n--- Validation Set Performance Metrics ---\")\nprint(f\"  Validation RMSE: {rmse_xgb_val:.4f}\")\nprint(f\"  Validation MAE:  {mae_xgb_val:.4f}\")\n\n# --- Visualization: True vs. Predicted Scores (Validation Set) ---\nprint(\"\\nGenerating True vs. Predicted plot for validation set...\")\nplt.figure(figsize=(8, 8))\nplt.scatter(y_val_split, val_preds_xgb, alpha=0.5, label=f'Predictions (RMSE={rmse_xgb_val:.3f})')\nmin_plot_val = min(y_val_split.min(), val_preds_xgb.min()) - 0.2\nmax_plot_val = max(y_val_split.max(), val_preds_xgb.max()) + 0.2\nplt.plot([min_plot_val, max_plot_val], [min_plot_val, max_plot_val], '--', color='red', linewidth=2, label='Ideal Line (y=x)')\nplt.xlabel(f\"True Scores ({SCORE_COLUMN_NAME}) - Validation Set\")\nplt.ylabel(\"Predicted Scores - Validation Set\")\nplt.title(f\"XGBoost (Embeddings): Validation Set Performance\")\nplt.xlim(min_plot_val, max_plot_val)\nplt.ylim(min_plot_val, max_plot_val)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.legend()\nplt.gca().set_aspect('equal', adjustable='box')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 10.1 Analyze Residuals (Validation Set) ---\nprint(\"\\nAnalyzing prediction residuals on the validation set...\")\nresiduals = y_val_split - val_preds_xgb # Calculate error: True - Predicted\n\nplt.figure(figsize=(8, 6))\nplt.scatter(val_preds_xgb, residuals, alpha=0.5)\nplt.axhline(y=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\nplt.xlabel(\"Predicted Scores (Validation)\")\nplt.ylabel(\"Residuals (True Score - Predicted Score)\")\nplt.title(\"Residual Plot (Validation Set)\")\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.legend()\nplt.show()\n\n# Optional: Histogram of residuals\nplt.figure(figsize=(8, 5))\nsns.histplot(residuals, kde=True, bins=20)\nplt.title('Distribution of Residuals (Validation Set)')\nplt.xlabel('Prediction Error')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 11. Retrain Final Model on Full Training Data & Calculate Training RMSE ---\n\nprint(f\"\\nRetraining final XGBoost model on the *entire* training dataset (embeddings)...\")\n\n# Determine the optimal number of boosting rounds\nbest_n_estimators = xgb_model.n_estimators\nif hasattr(xgb_model, 'best_iteration') and xgb_model.best_iteration is not None:\n    best_n_estimators = xgb_model.best_iteration + 1\n    print(f\"Using {best_n_estimators} boosting rounds based on early stopping result.\")\nelse:\n    print(f\"Using original n_estimators = {best_n_estimators}.\")\n\n# Initialize the FINAL model with potentially adjusted hyperparameters\nfinal_xgb_model = xgb.XGBRegressor(\n    objective='reg:squarederror',\n    n_estimators=best_n_estimators, # Use determined rounds\n    learning_rate=0.03,           # Use same potentially adjusted LR\n    max_depth=4,                  # Use same potentially adjusted depth\n    subsample=0.7,\n    colsample_bytree=0.7,\n    gamma=0.2,\n    reg_alpha=0.01,\n    random_state=SEED,\n    n_jobs=-1\n)\n\n# Train the final model on ALL training embedding features (X_train_features) and labels (y_train)\nfinal_xgb_model.fit(X_train_features, y_train, verbose=False)\nprint(\"Final model training on full training data complete.\")\n\n# --- Calculate RMSE on the TRAINING data itself ---\nprint(\"\\nCalculating performance of the final model on the training data...\")\n# Predict on the training embedding features\ntrain_preds_final = final_xgb_model.predict(X_train_features)\n\n# Calculate Training RMSE\ntraining_rmse_final = mean_squared_error(y_train, train_preds_final, squared=False)\n\nprint(\"\\n\" + \"=\"*50)\nprint(f\"  FINAL MODEL - **TRAINING DATA RMSE**: {training_rmse_final:.4f}\")\nprint(\"=\"*50)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 11.1 Visualize Feature Importance (Final Model) ---\nprint(\"\\nVisualizing feature importances from the final XGBoost model...\")\n\ntry:\n    plt.figure(figsize=(10, 8)) # Adjust size as needed\n    xgb.plot_importance(final_xgb_model, max_num_features=20, height=0.8) # Show top 20 features\n    plt.title('XGBoost Feature Importance (Top 20)')\n    # Note: Feature names will likely be f0, f1, ... for embeddings\n    plt.xlabel(\"F-score (Frequency)\")\n    plt.ylabel(\"Feature Index\")\n    plt.tight_layout()\n    plt.show()\nexcept Exception as e:\n    print(f\"Could not plot feature importance: {e}\")\n    # Fallback if plot_importance fails or if you want other types:\n    # try:\n    #     importances = final_xgb_model.feature_importances_\n    #     indices = np.argsort(importances)[::-1]\n    #     plt.figure(figsize=(10,8))\n    #     plt.bar(range(min(20, len(importances))), importances[indices[:20]])\n    #     plt.xticks(range(min(20, len(importances))), [f'f{i}' for i in indices[:20]], rotation=90)\n    #     plt.title('XGBoost Feature Importance (Top 20 - Importance Weight)')\n    #     plt.show()\n    # except Exception as e2:\n    #      print(f\"Could not plot feature importance manually either: {e2}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 12. Predict on Test Set & Clip Predictions ---\n\nprint(\"\\nGenerating predictions for the test set using the final model (embeddings)...\")\n# Use the final model to predict on the test embedding features (X_test_features)\ntest_predictions = final_xgb_model.predict(X_test_features)\nprint(f\"Generated {len(test_predictions)} predictions for the test set.\")\n\n# --- Clipping Predictions ---\nmin_score_train = y_train.min()\nmax_score_train = y_train.max()\nprint(f\"Clipping test predictions to the observed training score range: [{min_score_train:.2f}, {max_score_train:.2f}]\")\ntest_predictions_clipped = np.clip(test_predictions, min_score_train, max_score_train)\n\nclip_diff = np.sum(test_predictions != test_predictions_clipped)\nif clip_diff > 0:\n      print(f\"Clipping affected {clip_diff} out of {len(test_predictions)} predictions.\")\nelse:\n      print(\"Clipping did not change any prediction values.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 13. Create Submission File ---\n\nprint(f\"\\nCreating submission file with 'filename' and 'label' columns...\")\nprint(f\"Output path: {SUBMISSION_FILE}\")\n\nsubmission_df = pd.DataFrame({\n    'filename': test_df['filename'],\n    'label': test_predictions_clipped # Use 'label' for submission column name\n})\n\nif submission_df.shape[0] != sample_sub.shape[0]:\n     print(f\"Warning: Submission row count {submission_df.shape[0]} does not match sample submission row count {sample_sub.shape[0]}\")\n\nprint(f\"Submission DataFrame columns created: {list(submission_df.columns)}\")\nif list(submission_df.columns) != ['filename', 'label']:\n     print(f\"Warning: Created columns {list(submission_df.columns)} do not match expected ['filename', 'label']\")\n\nsubmission_df['label'] = submission_df['label'].astype(float)\nsubmission_df.to_csv(SUBMISSION_FILE, index=False)\n\nprint(f\"\\nSubmission file '{os.path.basename(SUBMISSION_FILE)}' created successfully!\")\nprint(\"--- Submission File Preview ---\")\nprint(submission_df.head())\n\nif os.path.exists(SUBMISSION_FILE):\n    print(f\"\\nFile confirmed present in {OUTPUT_DIR}\")\n    print(\"You can now commit your notebook and submit.\")\nelse:\n    print(f\"\\nERROR: Submission file was expected at {SUBMISSION_FILE} but not found!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SHL Grammar Scoring Engine - Report\n\n## 1. Overview\n*   **Objective:** To develop a regression model predicting a continuous grammar score (observed range: 1.0 to 5.0 based on the 'label' column) for spoken English audio samples (45-60 seconds). The model takes an audio file as input and outputs the predicted score.\n*   **Dataset:** Provided training set (444 samples with scores) and test set (195 samples for prediction, 204 in `test.csv`). Audio files in `.wav` format. *(Note: test.csv has 204 rows, ensure submission aligns if needed, though standard SHL usually uses the 195)*.\n*   **Evaluation Metric:** Root Mean Squared Error (RMSE) for internal model evaluation. The final reported score requires the **Training RMSE**. The competition leaderboard uses a separate metric where higher scores are better.\n\n## 2. Data Loading and Exploration\n*   **Data Source:** Loaded `train.csv` and `test.csv` from the Kaggle `/kaggle/input/shl-intern-hiring-assessment/Dataset/` directory. Audio files located in corresponding `audios/train/` and `audios/test/` subdirectories.\n*   **Score Column:** The target variable containing grammar scores was confirmed as the column named `label` in `train.csv`.\n*   **Score Distribution:** The scores in the training data range from 1.0 to 5.0, with a mean of approximately 3.97 and a standard deviation of 1.05. The distribution appears somewhat skewed towards the higher scores (4.0 and 5.0). *(Refine description based on the histogram visualization if desired)*.\n*   **Audio Properties:** Checked sample audio files. Confirmed durations were approximately 60 seconds. Confirmed sample rates were 16000 Hz and audio was mono, matching the `TARGET_SR` and preprocessing expectations.\n\n## 3. Preprocessing\n*   **Audio Handling:** All audio files were loaded using `librosa` with the `sr=TARGET_SR` (16000 Hz) and `mono=True` parameters, ensuring uniform sample rate and single-channel audio. Audio data was converted to `float32` NumPy arrays.\n*   **Speech-to-Text (ASR):** OpenAI's Whisper ASR model (size: `small`) was used to transcribe the audio content into text. This model was chosen for a better balance between accuracy and computational cost compared to `base`.\n*   **Caching:** Generated transcriptions were cached to CSV files (`train_transcriptions_small.csv`, `test_transcriptions_small.csv`) in `/kaggle/working/transcriptions/` to significantly speed up subsequent notebook runs.\n*   **Error Handling:** Files not found or encountering transcription errors resulted in an empty string transcription to prevent downstream issues.\n\n## 4. Feature Engineering\n*   **Method:** Sentence embeddings were generated using the `sentence-transformers` library to capture semantic meaning and context from the transcriptions, replacing the initial TF-IDF approach.\n*   **Model:** The pre-trained `'all-MiniLM-L6-v2'` model was used due to its good balance of performance and speed.\n*   **Process:** Each transcription (from both train and test sets) was passed through the SentenceTransformer model to produce a fixed-size dense vector embedding (384 dimensions for this model).\n*   **Result:** This resulted in dense NumPy arrays `X_train_features` (444, 384) and `X_test_features` (204, 384) used as input for the regression model.\n\n## 5. Methodology / Model Architecture\n*   **Model Choice:** An XGBoost Regressor (`xgboost.XGBRegressor`) was selected for modeling.\n*   **Rationale:** XGBoost is well-suited for structured data, including dense embeddings, and offers robust performance with built-in regularization and handling of potential non-linear relationships between features and the target score.\n*   **Hyperparameters:** Key hyperparameters included `n_estimators=300` (max), `learning_rate=0.03`, `max_depth=4`, `subsample=0.7`, `colsample_bytree=0.7`, `gamma=0.2`, `reg_alpha=0.01`. *(Note: These might benefit from further tuning via cross-validation)*.\n*   **Training Strategy:** Early stopping (`early_stopping_rounds=30`) was employed during initial training on a validation split to prevent overfitting and determine a reasonable number of boosting rounds (identified as 322 rounds in the final run).\n\n## 6. Training\n*   **Data Split:** The training embedding features (`X_train_features`) and corresponding labels (`y_train`) were split into an 80% training set and a 20% validation set (`random_state=42`).\n*   **Initial Training:** The XGBoost model was first trained on the 80% training split, using the 20% validation split to monitor RMSE and trigger early stopping.\n*   **Final Training:** The final XGBoost model was retrained on the **entire** training dataset (all 444 samples and their embedding features) using the hyperparameters from the initial model and the optimal number of estimators (322 rounds) determined by early stopping.\n\n## 7. Evaluation\n*   **Validation Performance:** The model trained on the 80% split achieved the following performance on the 20% validation set:\n    *   Validation RMSE: **1.0304**\n    *   Validation MAE: **0.8348**\n    *   The validation scatter plot shows the relationship between true and predicted scores on this held-out set. *(Briefly comment on the plot, e.g., \"While showing a positive correlation, the plot indicates considerable scatter, suggesting the model struggled somewhat to generalize perfectly on this specific validation split based on RMSE.\")*\n*   **REQUIRED Training Performance:** The final model, retrained on the entire training dataset, achieved the following performance *on that same training dataset*:\n    *   **Training RMSE: 0.1447** *(This is the mandatory value for submission)*\n    *   This very low training RMSE indicates the model with sentence embeddings fits the training data extremely well.\n\n## 8. Prediction Pipeline for Test Set\n*   The pipeline applied to the test data was:\n    1.  Load audio, resample to 16kHz, convert to mono.\n    2.  Transcribe using Whisper `small`.\n    3.  Generate sentence embeddings using `'all-MiniLM-L6-v2'`.\n    4.  Predict scores using the final XGBoost model trained on all training data embeddings.\n    5.  Clip predictions to the observed training score range [1.00, 5.00].\n    6.  Save results to `submission.csv` with 'filename' and 'label' columns.\n\n## 9. Conclusion & Discussion\n*   **Summary:** This notebook implements an end-to-end pipeline using Whisper `small` for ASR, Sentence-BERT (`all-MiniLM-L6-v2`) for feature extraction, and XGBoost for regression. The model achieved a very strong fit on the training data (Training RMSE: **0.1447**). The performance on the hidden test set yielded a leaderboard score of 0.459 (on the competition's specific metric).\n*   **Observations:** The switch from TF-IDF to Sentence-BERT significantly improved the model's ability to fit the training data (very low training RMSE), although the validation RMSE (1.0304) was higher than desired, suggesting potential sensitivity to the specific train/validation split or room for hyperparameter optimization. The final test score (0.459) indicates good performance but leaves a gap compared to the top leaderboard score (0.96), suggesting the evaluation metric might strongly favor features more directly related to grammatical correctness than semantic embeddings alone.\n*   **Limitations:**\n    *   ASR errors still possible even with Whisper `small`.\n    *   Sentence embeddings capture semantics well but might not be optimal for purely grammatical error detection as per the rubric.\n    *   Single train/validation split might not be fully representative; cross-validation would be more robust.\n    *   XGBoost hyperparameters were adjusted slightly but not systematically tuned.\n*   **Potential Future Improvements:**\n    *   **Linguistic Features:** Implement feature extraction using `spaCy` (POS tags, dependency relations, sentence structure stats) to directly target grammatical aspects, either replacing or combining with SBERT embeddings.\n    *   **Hyperparameter Tuning:** Use `RandomizedSearchCV` or `GridSearchCV` with K-Fold Cross-Validation to optimize XGBoost (or other models like LightGBM) for the chosen feature set (embeddings, linguistic, or combined).\n    *   **Advanced Embeddings/Models:** Explore larger Sentence-BERT models or even fine-tuning language models if compute resources allow.\n    *   **Ensembling:** Combine predictions from models trained on different feature sets (e.g., SBERT vs Linguistic) or different algorithms.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}